---
title: "Final Project"
author: "Noe Arambula"
date: '2022-04-22'
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
  pdf_document:
    toc: true
---

# Introduction

I will be constructing a model that predicts the price of an Airbnb depending on several variables that will be explained further in the following section. Since predicting price gives a continuous outcome I will be fitting and using my data on models best suited for regression. I will start by explaining a little bit about what Airbnb is and what they do and then go on to explaining why this model may be useful. Once we have a good idea about the kind of data we may be using I will load in my data set and begin data cleaning/splitting. Next, I will move on to exploratory data analysis to get a better understanding of the specific data we will be working with. Following this I will begin my model construction process and lastly, I will test my final model on testing data which will not be used throughout the model construction process.

## What is Airbnb?

[Airbnb](https://www.airbnb.com/) in a company that is operated completely online to allow travelers to book accommodations anywhere in the world. Anyone can make an account online in order to either post a listing or a "stay" that people can rent or to sign up and book different accommodations, once you request to stay somewhere the host must approve your stay. You can think of airbnb as a kind of platform or marketplace where people go to post an offer for or rent an accommodation.

Airbnb is mainly focused on home-stays, so many people rent their home or part of their home to travelers that need a place to stay. You simply go online type the dates of your vacation/stay, the location, and the amount of people that will be with you similar to booking a hotel. Then many different listings will pop up with different prices depending on the type of accommodation, location and other similar factors. You can also book through your phone as they have a mobile app.

## Why Might this Model be Useful?

This model could help hosts that want to post a new accommodation decide how much they want to list their place for. Given a certain amount of information about the price this model will tell them about how much other similar places are renting for on average.

## Loading Packages and Data set

```{r Loading Data, message=FALSE}
library(yardstick)
library(ISLR)
library(ISLR2)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(ranger)
library(discrim)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(corrr)
library(corrplot)
library(ggthemes)
library(readr)
library(corrr)
library(klaR)
library(MASS)
library(dplyr)
library(glmnet)

tidymodels_prefer()
set.seed(777)
```

I intend to use a data set from insideairbnb.com which sources the data from publicly available information on the official Airbnb site. The data-set includes information about the hosts of the living space being offered, the living space/listing itself, and reviews. The data set I will use will be strictly from the Los Angeles area. There are 33,330 observations and 74 predictors however I edited the .csv and took out some predictors as many were not necessary such as some urls to listings and description of the host/listings. The dataset has both numeric and character/string data variables as well categorical and continuous variables. Most of the data is continuous and I will be able to change the data I need into continuous variables in the Data Cleaning section

*Note: I have a code book with descriptions of all variables used in my github*

I converted the dates to a standard format and converted bathrooms_text to numeric which just changed for example 1 bath to 1 (dropping bath for all columns)

```{r warning=FALSE}
airbnb <- read_csv("airbnb_listings_final.csv", 
    col_types = cols(host_since = col_date(format = "%m/%d/%Y"),
                     first_review = col_date(format = "%m/%d/%Y"), 
                     last_review = col_date(format = "%m/%d/%Y"),
                     bathrooms_text = col_number()))
```

# Data Cleaning

The data set I am using already has standardized names for all variables so there is no need to use janitor to clean them up but there were still some things to clean up. After removing many variables and loading in the data set I realized there was still a few things I should take out since they are not very relevant to predicting price, after this I have a total of 31 predictors

```{r}
airbnb <- airbnb %>% 
  select(-amenities, -host_name, -neighbourhood_cleansed)
```

Convert Dates to Numeric this is so I can see if date a host joined and the dates of their reviews affects how they price their listing. I can I am able to now use these in the EDA and model building below

```{r}
airbnb$first_review <- as.numeric(airbnb$first_review)
  
airbnb$last_review <- as.numeric(airbnb$last_review)

airbnb$host_since <- as.numeric(airbnb$host_since)
```

Converting True/False Values to 0 and 1 (0 for false and 1 for true). Again this allows me to use these predictors in my continuous/linear regression analysis and models below

```{r}
airbnb$host_is_superhost <- as.numeric(airbnb$host_is_superhost)

airbnb$host_has_profile_pic <- as.numeric(airbnb$host_has_profile_pic)

airbnb$host_identity_verified <- as.numeric(airbnb$host_identity_verified)

airbnb$instant_bookable <- as.numeric(airbnb$instant_bookable)
```

I will change property type to 1 or 2 depending on if it is an entire home(1) or just a room being(2) rented so I can see how price affects each type of accommodation

```{r warning=FALSE}
airbnb$property_type [airbnb$property_type == 'Entire home/apt'] <- 1
airbnb$property_type [airbnb$property_type == 'Private room'] <- 2

airbnb$property_type <- as.integer(airbnb$property_type)
```

Lastly I will convert host_response_time to 1 2 3 or 4 depending on how fast they respond 1 being the fastest and 4 being the highest( from 'within an hour', 'within a few hours'. 'within an day', and 'a few days or more' respectively) I am leaving all of these variables in to see if the hosts engagement with the app effects the price they set in any way.

```{r warning=FALSE}
airbnb$host_response_time [airbnb$host_response_time == 'within an hour'] <- 1
airbnb$host_response_time [airbnb$host_response_time == 'within a few hours'] <- 2
airbnb$host_response_time [airbnb$host_response_time == 'within an day'] <- 3
airbnb$host_response_time [airbnb$host_response_time == 'a few days or more'] <- 4

airbnb$host_response_time <- as.integer(airbnb$host_response_time)
```

*note: the point of converting all to numeric types was so I can include them in my EDA otherwise I would convert to dummy variables in my recipe's*

I now want to check to see that all my data looks good and ready to go

```{r}
head(airbnb)
```

*Note: I will be dealing with missing values my imputing in my recipe's later*

## Data Splitting

I will use the 80-20 split for my training and testing data respectively. I stratified my data on the number of guests that can be accommodated in each listing as I feel that will be the biggest factor in predicting price and I can split each equally

```{r}
set.seed(777)
airbnb_split <- initial_split(airbnb, prop = 0.8, strata = accommodates)

airbnb_train <- training(airbnb_split)
airbnb_test <- testing(airbnb_split)
```

The training data set has 26662 observation and the testing set has 6667 observations for a total of 33329 observations

# Exploratory Data Analysis (EDA)

I will be using the training set for this section where each observation is a different listing.

I am going to start off by looking at boxplots of a few different variables that I am most worried about affecting the data set

```{r}
airbnb_train %>% 
  ggplot(aes(x = accommodates)) +
  geom_boxplot()

airbnb_train %>% 
  ggplot(aes(x = price, y = property_type)) +
  geom_boxplot()

airbnb_train %>% 
  ggplot(aes(x = bedrooms)) +
  geom_boxplot()
```

From Looking at these two boxplots it seems as though we have some heavy outliers for price and a few for the amount of people being accommodated and sure enough once I went back to look at the data and organized it by price and accommodates I saw that there are a few listings that are up to 25000\$!

The vast majority of listings however, are under 500\$ and so I will be filtering out these observations from my training and testing sets now in order to continue my EDA and get a true sense of what my data looks like. Filtering out price coincidentally filters out the outliers for other predictors as well such as accommodates, and the number of bedrooms.

```{r}
 airbnb_train <- airbnb_train %>%
   filter(price < 500)
 
airbnb_test <- airbnb_test %>%
  filter(price < 500)

airbnb_train <- airbnb_train %>%
  filter(accommodates < 11)

airbnb_test <- airbnb_test %>%
  filter(accommodates < 11)

dim(airbnb_train)
dim(airbnb_test)
```

airbnb_train now has 24270 observations loss of about 2200 observations and the testing set has 6059 observations a loss of about 550 observations so they both still represent about 80% and 20% of our remaining data set of 30329 total observations\

```{r}
airbnb_train %>% 
  ggplot(aes(x= price)) +
  geom_boxplot()
```

This looks much better than before and we can also take a look at a histogram of both price and accommodates

```{r}

airbnb_train %>% 
  ggplot(aes(reorder(accommodates,price),price)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    title = "Price by Number of People Accommodated",
    x = "Max Number of People That Can be Accomodated"
  )

airbnb_train %>% 
  ggplot(aes(reorder(property_type,price),price)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    title = "Price by Property Type",
    x = "Property Type 1 = entire unit/house, 2 = room only"
  )
```

Here we see a strong correlation between price and number of people that can be accommodated. We can see that even though there are airbnbs where the number of people allowed to stay is low the price can go very high if you would like to rent a more luxurious accommodation. Airbnb has many different types of accommodations even though it splits accommodation type into either an entire house/apt or just a bedroom the accommodations can be anything from a regular suburban house Inglewood or glass dome in the middle of the Mojave desert!

We must also take note that there are a significant amount of missing data for property type. Which makes me think that maybe I should not use it as clearly a whole accommodation should have a higher price than just renting a room.

Before moving forward I also would like to say that the price is based on a per night basis. The data was collected straight from the airbnb website and each observation was collected on the same day. At the end of each stay the guest rates the accommodation based on several things such as overall experience, cleanliness and value all of which have been included in the data set as observations. Below is a photo of the airbnb app so you can get a sense of how it works.

![](images/IMG_7330.PNG){width="454"}

```{r}
airbnb_train %>% 
  ggplot(aes(x = price)) +
  geom_histogram()

airbnb_train %>% 
  ggplot(aes(x = accommodates)) +
  geom_histogram()

```

We can see that the median price lies at around 100 and median number of people that a listing can accommodate is about two and that we have a leftward skew for both variables

Now lets take a look at a correlation matrix between a few of the predictors

```{r}
airbnb_train %>% 
  select(host_since,host_response_time,host_is_superhost,accommodates,
         price, number_of_reviews, review_scores_rating, property_type) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(type = "lower", diag = FALSE)
```

Here we see again the relationship between price and accommodates variables, interestingly there is a negative correlation between number of reviews and the length of time since a host has joined which makes sense as those who have joined recently will not have as many people that have stayed in their accommodation. A super host is someone who consistently is supposed to go above and beyond to meet guests needs and be very engaged with them which makes sense why we would see a correlation between super host and review rating.

# Model Building

Now for my model building I will be trying out the following models on my data:

1.  Linear Regression Model
2.  Random Forest Model
3.  Ridge Regression Model
4.  Boosted Tree Model

I will organize my code into three categories Building The Model, running/testing the models, and model analysis

## Building Recipe/folds and Final Data

From the EDA I saw that their was a significant amount of missing values so I decided to look back through observations more carefully and saw there was a lot of missing data for all the review scores observations and have decided to filter out all observations with NA as I should still have more than enough to construct a model.

```{r}
final_train <- airbnb_train %>% drop_na()
final_test <- airbnb_test %>% drop_na()

dim(final_train)
dim(final_test)
```

I assigned this data to a new set, This left us with 9354 obs. for the training set down from 24270 obs. leaving us with the an 80 20 split still and 2397 observations in the testing set down from 6059 obs.

I can now start the model building process but I double checked the price count just to double check the distribution stayed the same

```{r}
airbnb_train %>% 
 ggplot(aes(x = price)) +
 geom_histogram()
```

## Building/Running The Models

### Linear Regression

I will first setup **linear regression model** starting with a recipe:

```{r}
airbnb_recipe <- recipe(price ~ . , data = final_train) %>% # predict price with all predictors
  step_normalize(all_predictors()) # this centers and scales all variables

```

I do not believe there was a strong enough correlation between any of the predictors that warranted creating an interaction term and I converted all T/F and categorical predictors into numerical categories in data cleaning so there is no need for any dummy coding

I now create a linear regression object and workflow :

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")

lm_wf <- workflow() %>% 
  add_recipe(airbnb_recipe) %>% 
  add_model(lm_model)
```

I now fit my model:

```{r}
airbnb_fit <- fit(lm_wf, final_train)
```

### Random Forest Model

I will now set up a random forest model and workflow:

```{r}
set.seed(777)
rf_mod <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(airbnb_recipe) %>%
  add_model(rf_mod)
```

Mtry is the number of predictors that will be randomly chosen at each split of the tree models thus it cannot be lower than 1 since then no predictors would be chosen or greater than 31 since we have 31 predictors. If mtry = 31 that would represent a bagging model.

Trees represents the number of trees that will be used in each model

min_n represents the minimum number of of data points needed at each node/tree end that is needed in order to split it further into another tree

I will now setup a grid for each hyperparameter we are tuning and fold the data for cross validation:

```{r}
param_grid <- grid_regular(mtry(range = c(6,28)), 
                     trees(range = c(2, 5)), 
                     min_n(), levels = c(10,10,10))

# Fold
airbnb_fold <- vfold_cv(final_train, v = 5, repeats = 5, strata = price)

```

Now lets fit the tuned model:

```{r eval=FALSE}
set.seed(777)
tune_res_rf <- tune_grid(
  rf_wf, 
  resamples = airbnb_fold, 
  grid = param_grid,
)
```

*Note: I ran and saved this model outside of the rmd in the save models r file and will just load the model in during analysis since this took about 15-20min to run*

### Boosted Tree Model

First we Set up the Boosted Tree Model and workflow:

```{r}
boost_spec <- boost_tree(trees = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_wf <- workflow() %>% 
  add_recipe(airbnb_recipe) %>% 
  add_model(boost_spec)
```

Tuning trees lets us try models different number trees and tuning tree depth is a parameter that controls the number of splits of each tree

Next we set up the grid for trees and tree_depth:

```{r}
boost_grid <- grid_regular(trees(range = c(50, 350)), 
                     tree_depth(), levels = c(4,8))
```

Now Fit the tuned model:

```{r eval=FALSE}
set.seed(777)
tune_res_boost <- tune_grid(
  boost_wf,
  resamples = airbnb_fold, 
  grid = boost_grid
)
```

*Note: I ran and saved this model outside of the rmd in the save models r file and will just load the model in during analysis since this took about 15min to run*

## Model Analysis

I will first load in the models from my .rda file:

```{r}
load(file = "tunedmodels.rda")
```

### Linear Regression Model

I will predict price using my data and see how it fairs against the actual training data

```{r}
multi_metric <- metric_set(rmse, rsq, mae)

airbnb_predict <- predict(airbnb_fit, final_train) %>% 
  bind_cols(final_train %>% select(price))

multi_metric(airbnb_predict, truth = price, estimate = .pred)
```

Looking at the rmse and R\^2 values we see that the model is not great at predicting the price. With an R\^2 value just above .5 this means that the model only explains about 50% of the variance in the response variable around the mean. So, about half of the values are not predicted well. The rmse of 70 is not good either since most listing are around the 200\$ price point.

### Random Forest Model

First Lets look at some performance metrics:

```{r}
collect_metrics(tune_res_rf, metric = ) %>%   
  arrange(mean)
```

We can select the best from these now:

```{r}
show_best(tune_res_rf, metric = "rsq")
show_best(tune_res_rf, metric = "rmse")
```

The best rmse value is 65.24286 and the best rsq value is 0.5969

We can also look at a graph of the performance:

```{r}
autoplot(tune_res_rf)
```

From this graph it is clear that the more trees were used the better the rsq and rmse values got which is expected. However, the rsq value is still not great meaning we should use more trees in our model. The model also performed best using about 15 predictors

### Boosted Tree Model

First Lets look at some performance metrics

```{r}
collect_metrics(tune_res_boost) %>%   
  arrange(mean)

```

Lets select the best models from these:

```{r}
show_best(tune_res_boost, metric = "rsq")
show_best(tune_res_boost, metric = "rmse")
```

Looking at this the best rsq value for the boosted tree model was 0.6561402 and the best rmse value was 60.23864 this is very close to the random forest model but a bit better so I would choose this model as my final model.

Lets look at a plot fo the tuned parameters:

```{r}
autoplot(tune_res_boost)
```

Here we can see that the best tree depth value would be 4 which gives us a spike in the rsq and decline in the rmse. The number of trees here did not affect the data as much as I thought it would they are all similar however 50 trees performs the best.

# Final Model and Analysis

I will now construct the final model using the best model from the boosted tree tuned models:

```{r}
best_boosted <- select_best(tune_res_boost, metric = 'rsq')
final_wf <- finalize_workflow(boost_wf, best_boosted)

final_fit <- fit(final_wf, final_train)
```

We can now see how well our final model did vs the testing set:

```{r}
airbnb_metric <- metric_set(rsq)

model_test_predictions <- predict(final_fit, new_data = final_test) %>% 
  bind_cols(final_test %>% select(price)) 


model_test_predictions %>% 
  airbnb_metric(truth = price, estimate = .pred)

augment(final_fit, new_data = final_test) %>%
  rmse(truth = price, estimate = .pred)
```

We get an rsq of 0.64 which is similar to the one we got using the training set of 0.6561402 and an rmse of 58 which is also close but a little better to the previews rmse of 60.23864

# Conclusion

Overall, our model were not the best at predicting price however the boosted tree model was the best and when transferred over to the testing data it performed relatively the same as it did with the training set. While more trees usually means overfitting the amount of trees the model performed best at was at 50 trees rather than anything higher as I had tuned up to 350 trees for this model. This model may have performed a bit better than the others as predicting price with so many predictors would most likely be too complicated for a regular lm model.

If I were to go back I would try tuning other various amounts of trees for the random forest model or try a different model altogether like maybe ridge regression. Overall this was a great way to test out different models on real world data and navigate all the errors and problems that come with working with real life data. While our models were not the best at predicting price for Airbnb's I feel as though I learned a lot about the data and how I could improve upon this project in the future.
